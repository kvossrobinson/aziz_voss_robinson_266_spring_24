{"cells":[{"cell_type":"markdown","metadata":{"id":"hJSSHyX4IEt7"},"source":["# README\n","\n","**Purpose**\n","\n","Evaluate a QA model on domain-specific QA data\n","\n","**How-To**\n","\n","Make any updates in the \"Update Each Run\" section before running all cells. Places to take particular care:\n","\n","\n","*   If you don't want to overwrite the generic QA eval results, make sure to save them off in a \"generic_eval\" folder within your model folder\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZkSYjwDLYx_r"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"Cq9M83c4IGGI"},"source":["## Update Each Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiMBrw5mYqNM"},"outputs":[],"source":["# Katherine: Make sure to look at these and edit at least the path\n","\n","# Model you're evaluating\n","input_model_path = '/content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt'\n","\n","# Where to put the evaluation outputs\n","dir = input_model_path # Note: if you do use the same path, save off the generic eval you get after running the original QA code if you want to reference it later\n","\n","# Run Name\n","name = 'Eval: TDAPT SOT'"]},{"cell_type":"markdown","metadata":{"id":"_Zp_u9cFYqEB"},"source":["## Should Not Need Regular Changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfoqKm_cIGRN"},"outputs":[],"source":["# Pathing\n","path_base = '/content/drive/MyDrive/266/source_of_truth/' # Needs to contain trainer_qa.py and utils_qa.py\n","\n","# Validation file - used for domain-specific QA evaluation\n","val_file = '/content/drive/MyDrive/266/source_of_truth/data/squad.film.test.squad_format.json'\n","\n","# Hyperparameters\n","\n","# Are you training?\n","do_train_flag = False # Skipping this\n","\n","# Are you evaluating?\n","do_eval_flag = True\n","\n","# Number of train epochs\n","epoch_count = 2 # Irrelevant but leaving to avoid glitches\n","\n","# Number of save steps\n","save_step_count = 1000 # Irrelevant but leaving to avoid glitches\n","\n","# Number of eval steps\n","eval_step_count = 1000 # Irrelevant but leaving to avoid glitches\n","\n","# Number of logging steps\n","logging_step_count = eval_step_count # Irrelevant but leaving to avoid glitches\n","\n","# Overwrite Output Dir - MUST be False if you're training from a checkpoint\n","overwrite = True # Irrelevant but leaving to avoid glitches"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16391,"status":"ok","timestamp":1712455321324,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"},"user_tz":300},"id":"eDCaPrKR2vYg","outputId":"95d19b13-09f8-4e35-c5f0-f24b82cfea8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-TpolrMK2D18","executionInfo":{"status":"ok","timestamp":1712456626908,"user_tz":300,"elapsed":1305587,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"outputId":"389de139-a93f-4f82-ceb9-3a1f6bdf92ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m861.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m886.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q evaluate\n","!pip install -q datasets\n","!pip install -q accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rAFpu6S0iMU"},"outputs":[],"source":["# Making the necessary imports\n","import logging\n","import os\n","import sys\n","import warnings\n","from dataclasses import dataclass, field\n","from typing import Optional, Tuple, Union\n","\n","import datasets\n","import evaluate\n","from datasets import load_dataset\n","\n","# Importing the helper python files\n","import sys\n","# Append the directory to your python path using sys\n","sys.path.append(path_base)\n","from trainer_qa import QuestionAnsweringTrainer\n","from utils_qa import postprocess_qa_predictions\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForQuestionAnswering,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PreTrainedTokenizerFast,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"raa1m12O8ZAJ"},"outputs":[],"source":["# Setting logger\n","logger = logging.getLogger(__name__)"]},{"cell_type":"markdown","metadata":{"id":"V9bezCWt8i1U"},"source":["# Classes"]},{"cell_type":"markdown","metadata":{"id":"yEDG4I4Aza6r"},"source":["## ModelArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYot5aWb3pul"},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    token: str = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n","                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n","            )\n","        },\n","    )\n","    use_auth_token: bool = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n","        },\n","    )\n","    trust_remote_code: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": (\n","                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n","                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n","                \"execute code present on the Hub on your local machine.\"\n","            )\n","        },\n","    )"]},{"cell_type":"markdown","metadata":{"id":"BXEoEZ2YzdhF"},"source":["## DataTrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw0m7xQ08klO"},"outputs":[],"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    dataset_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )\n","    dataset_config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n","    )\n","    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n","    validation_file: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n","    )\n","    test_file: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","    preprocessing_num_workers: Optional[int] = field(\n","        default=None,\n","        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n","    )\n","    max_seq_length: int = field(\n","        default=384,\n","        metadata={\n","            \"help\": (\n","                \"The maximum total input sequence length after tokenization. Sequences longer \"\n","                \"than this will be truncated, sequences shorter will be padded.\"\n","            )\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": (\n","                \"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when\"\n","                \" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU).\"\n","            )\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    version_2_with_negative: bool = field(\n","        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n","    )\n","    null_score_diff_threshold: float = field(\n","        default=0.0,\n","        metadata={\n","            \"help\": (\n","                \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n","                \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n","                \"Only useful when `version_2_with_negative=True`.\"\n","            )\n","        },\n","    )\n","    doc_stride: int = field(\n","        default=128,\n","        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\n","    )\n","    n_best_size: int = field(\n","        default=20,\n","        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n","    )\n","    max_answer_length: int = field(\n","        default=30,\n","        metadata={\n","            \"help\": (\n","                \"The maximum length of an answer that can be generated. This is needed because the start \"\n","                \"and end predictions are not conditioned on one another.\"\n","            )\n","        },\n","    )\n","\n","    def __post_init__(self):\n","        if (\n","            self.dataset_name is None\n","            and self.train_file is None\n","            and self.validation_file is None\n","            and self.test_file is None\n","        ):\n","            raise ValueError(\"Need either a dataset name or a training/validation file/test_file.\")\n","        else:\n","            if self.train_file is not None:\n","                extension = self.train_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n","            if self.validation_file is not None:\n","                extension = self.validation_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n","            if self.test_file is not None:\n","                extension = self.test_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\""]},{"cell_type":"markdown","metadata":{"id":"zSAzTcpoD1q7"},"source":["## MetricsLogger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKOm_fc5D1x_"},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class MetricsLogger(TrainerCallback):\n","    \"A logger that will store metrics after each evaluation.\"\n","    def __init__(self):\n","        self.metrics = []\n","\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            # You can add more metrics here if needed\n","            self.metrics.append({\n","                \"step\": state.global_step,\n","                \"loss\": logs.get(\"loss\", None),\n","                \"eval_loss\": logs.get(\"eval_loss\", None),\n","            })\n","\n","# Initialize your logger\n","metrics_logger = MetricsLogger()"]},{"cell_type":"markdown","metadata":{"id":"oMI_Qfqm9r63"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"-S-j6jD5MT9T"},"source":["## Creating Args"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"testFWWKDvxt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712457566256,"user_tz":300,"elapsed":105,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"outputId":"6a5ee2a2-1db2-4f0d-f9ae-584e466f5a58"},"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO|training_args.py:1902] 2024-04-07 02:39:26,139 >> PyTorch: setting up devices\n","[INFO|training_args.py:1611] 2024-04-07 02:39:26,163 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# Creating instances of the three required classes\n","\n","model_args = ModelArguments(model_name_or_path = input_model_path)\n","\n","data_args = DataTrainingArguments(validation_file = val_file)\n","\n","# This class is imported above; we're not making it like the other two\n","training_args = TrainingArguments(output_dir = dir,\n","                                  do_train = do_train_flag,\n","                                  do_eval = do_eval_flag,\n","                                  num_train_epochs = epoch_count,\n","                                  save_strategy = \"steps\",\n","                                  evaluation_strategy = \"steps\",\n","                                  save_steps = save_step_count,\n","                                  eval_steps = eval_step_count,\n","                                  logging_steps = logging_step_count,\n","                                  save_total_limit = 2,\n","                                  per_device_train_batch_size = 16,\n","                                  per_device_eval_batch_size = 32,\n","                                  overwrite_output_dir = overwrite,\n","                                  logging_first_step = True,\n","                                  run_name = name\n","                                  )"]},{"cell_type":"markdown","metadata":{"id":"hSx5MNPjMWMV"},"source":["## Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["308259065b8b493b87136659d3256b00","8ebb34079eab46cab5a95a7fab546f67","d4e9c0ccbe584aa694917290976062d1","acf3d247c7cf4e9195339e3bb5e29456","aa742d4fe1b644419a4893547be8311c","91c481c1150a455cb6ec94b4a8eafc96","1d0dc4710e7248b29be8d5a8415b45ad","430bfe6e5ac44b3793083f18dcf8661d","8da2cbd08cfc413f95373d1dd18f19db","6a7f4e02cebd42419626c07aa630e803","516a30b2a9ea496884d83019e5b4773d","ae24a52c3026423abeb218f9e398fd82","0d305fbb24564b4c8d74a0bdad998e47","b2197d53ca694b13806e17178380f327","ad7ec11cea374a95be1c20c1a40fb052","98d6a00fafeb40928686a696fee93274","5968b0ca470e49d4b5f0766a150e9dec","e1a8fe0a0e384f1caac8f8c7193aaf88","5f01d4194a4e4cabb4ae68f32bd196d3","a0ac1f4c04d346e39ec0e0ce72ed7770","d1b4f4bc675d478ab197b4009fc12daa","6fb8b1888874464eb171e9bb1c593ae7"]},"id":"zO_9kABs8sRA","executionInfo":{"status":"ok","timestamp":1712457590425,"user_tz":300,"elapsed":23720,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"outputId":"af93fb5f-3574-440d-ad6b-47d249ed65c9"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=1000,\n","evaluation_strategy=steps,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt/runs/Apr07_02-39-26_835250cc425c,\n","logging_first_step=True,\n","logging_nan_inf_filter=True,\n","logging_steps=1000,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=2,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=32,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=Eval: TDAPT SOT,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=1000,\n","save_strategy=steps,\n","save_total_limit=2,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-9ae907479342e186\n","INFO:datasets.builder:Using custom data configuration default-9ae907479342e186\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n","INFO:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","[INFO|configuration_utils.py:726] 2024-04-07 02:39:27,259 >> loading configuration file /content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt/config.json\n","[INFO|configuration_utils.py:791] 2024-04-07 02:39:27,263 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt\",\n","  \"architectures\": [\n","    \"RobertaForQuestionAnsweringCustom\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,447 >> loading file vocab.json\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,449 >> loading file merges.txt\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,452 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,452 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,454 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2044] 2024-04-07 02:39:27,455 >> loading file tokenizer_config.json\n","[INFO|modeling_utils.py:3254] 2024-04-07 02:39:27,761 >> loading weights file /content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt/model.safetensors\n","[INFO|modeling_utils.py:3992] 2024-04-07 02:39:34,216 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n","\n","[INFO|modeling_utils.py:4000] 2024-04-07 02:39:34,219 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at /content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["******I am in the with statement******\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/755 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"308259065b8b493b87136659d3256b00"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Caching processed dataset at /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9ca114652f519d43.arrow\n","INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-9ae907479342e186/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9ca114652f519d43.arrow\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:759] 2024-04-07 02:39:35,921 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-07 02:39:35,932 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-07 02:39:35,933 >>   Num examples = 755\n","[INFO|trainer.py:3381] 2024-04-07 02:39:35,935 >>   Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24/24 00:08]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:utils_qa:Post-processing 755 example predictions split into 755 features.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/755 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae24a52c3026423abeb218f9e398fd82"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:utils_qa:Saving predictions to /content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt/eval_predictions.json.\n","INFO:utils_qa:Saving nbest_preds to /content/drive/MyDrive/266/source_of_truth/baseline_models/tdapt/eval_nbest_predictions.json.\n"]},{"output_type":"stream","name":"stdout","text":["***** eval metrics *****\n","  eval_exact_match        =    60.1325\n","  eval_f1                 =    71.9462\n","  eval_runtime            = 0:00:08.92\n","  eval_samples            =        755\n","  eval_samples_per_second =     84.607\n","  eval_steps_per_second   =      2.689\n"]},{"output_type":"stream","name":"stderr","text":["[INFO|modelcard.py:450] 2024-04-07 02:39:50,215 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n"]}],"source":["def main():\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    # Commenting this out in an attempt to pass the arguments directly\n","    # parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","    # if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n","    #     # If we pass only one argument to the script and it's the path to a json file,\n","    #     # let's parse it to get our arguments.\n","    #     model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n","    # else:\n","    #     model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n","\n","    if model_args.use_auth_token is not None:\n","        warnings.warn(\n","            \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\",\n","            FutureWarning,\n","        )\n","        if model_args.token is not None:\n","            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n","        model_args.token = model_args.use_auth_token\n","\n","    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n","    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n","    send_example_telemetry(\"run_qa\", model_args, data_args)\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    if training_args.should_log:\n","        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n","        transformers.utils.logging.set_verbosity_info()\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n","        + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n","    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n","    # (the dataset will be downloaded automatically from the datasets Hub).\n","    #\n","    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n","    # 'text' is found. You can easily tweak this behavior (see below).\n","    #\n","    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n","    # download the dataset.\n","    if data_args.dataset_name is not None:\n","        # Downloading and loading a dataset from the hub.\n","        raw_datasets = load_dataset(\n","            data_args.dataset_name,\n","            data_args.dataset_config_name,\n","            cache_dir=model_args.cache_dir,\n","            token=model_args.token,\n","        )\n","    else:\n","        data_files = {}\n","        if data_args.train_file is not None:\n","            data_files[\"train\"] = data_args.train_file\n","            extension = data_args.train_file.split(\".\")[-1]\n","\n","        if data_args.validation_file is not None:\n","            data_files[\"validation\"] = data_args.validation_file\n","            extension = data_args.validation_file.split(\".\")[-1]\n","        if data_args.test_file is not None:\n","            data_files[\"test\"] = data_args.test_file\n","            extension = data_args.test_file.split(\".\")[-1]\n","        raw_datasets = load_dataset(\n","            extension,\n","            data_files=data_files,\n","            # Per Hisham's advice, you must comment this out when using a local dataset\n","            # field=\"data\",\n","            # cache_dir=model_args.cache_dir,\n","            # token=model_args.token,\n","        )\n","    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n","    # https://huggingface.co/docs/datasets/loading_datasets.\n","\n","    # Load pretrained model and tokenizer\n","    #\n","    # Distributed training:\n","    # The .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        token=model_args.token,\n","        trust_remote_code=model_args.trust_remote_code,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=True,\n","        revision=model_args.model_revision,\n","        token=model_args.token,\n","        trust_remote_code=model_args.trust_remote_code,\n","    )\n","    model = AutoModelForQuestionAnswering.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        token=model_args.token,\n","        trust_remote_code=model_args.trust_remote_code,\n","    )\n","\n","    # Tokenizer check: this script requires a fast tokenizer.\n","    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n","        raise ValueError(\n","            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n","            \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n","            \" this requirement\"\n","        )\n","\n","    # Preprocessing the datasets.\n","    # Preprocessing is slightly different for training and evaluation.\n","    if training_args.do_train:\n","        column_names = raw_datasets[\"train\"].column_names\n","    elif training_args.do_eval:\n","        column_names = raw_datasets[\"validation\"].column_names\n","    else:\n","        column_names = raw_datasets[\"test\"].column_names\n","\n","    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n","    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n","    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n","\n","    # Padding side determines if we do (question|context) or (context|question).\n","    pad_on_right = tokenizer.padding_side == \"right\"\n","\n","    if data_args.max_seq_length > tokenizer.model_max_length:\n","        logger.warning(\n","            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n","            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n","        )\n","    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n","\n","    # Training preprocessing\n","    def prepare_train_features(examples):\n","        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","        # left whitespace\n","        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n","\n","        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","        # in one example possible giving several features when a context is long, each of those features having a\n","        # context that overlaps a bit the context of the previous feature.\n","        tokenized_examples = tokenizer(\n","            examples[question_column_name if pad_on_right else context_column_name],\n","            examples[context_column_name if pad_on_right else question_column_name],\n","            truncation=\"only_second\" if pad_on_right else \"only_first\",\n","            max_length=max_seq_length,\n","            stride=data_args.doc_stride,\n","            return_overflowing_tokens=True,\n","            return_offsets_mapping=True,\n","            padding=\"max_length\" if data_args.pad_to_max_length else False,\n","        )\n","\n","        # Since one example might give us several features if it has a long context, we need a map from a feature to\n","        # its corresponding example. This key gives us just that.\n","        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","        # The offset mappings will give us a map from token to character position in the original context. This will\n","        # help us compute the start_positions and end_positions.\n","        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","        # Let's label those examples!\n","        tokenized_examples[\"start_positions\"] = []\n","        tokenized_examples[\"end_positions\"] = []\n","\n","        for i, offsets in enumerate(offset_mapping):\n","            # We will label impossible answers with the index of the CLS token.\n","            input_ids = tokenized_examples[\"input_ids\"][i]\n","            cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","            sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","            # One example can give several spans, this is the index of the example containing this span of text.\n","            sample_index = sample_mapping[i]\n","            answers = examples[answer_column_name][sample_index]\n","            # If no answers are given, set the cls_index as answer.\n","            if len(answers[\"answer_start\"]) == 0:\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","            else:\n","                # Start/end character index of the answer in the text.\n","                start_char = answers[\"answer_start\"][0]\n","                end_char = start_char + len(answers[\"text\"][0])\n","\n","                # Start token index of the current span in the text.\n","                token_start_index = 0\n","                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n","                    token_start_index += 1\n","\n","                # End token index of the current span in the text.\n","                token_end_index = len(input_ids) - 1\n","                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n","                    token_end_index -= 1\n","\n","                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                    tokenized_examples[\"start_positions\"].append(cls_index)\n","                    tokenized_examples[\"end_positions\"].append(cls_index)\n","                else:\n","                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n","                    # Note: we could go after the last offset if the answer is the last word (edge case).\n","                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                        token_start_index += 1\n","                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                    while offsets[token_end_index][1] >= end_char:\n","                        token_end_index -= 1\n","                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","\n","        return tokenized_examples\n","\n","    if training_args.do_train:\n","        if \"train\" not in raw_datasets:\n","            raise ValueError(\"--do_train requires a train dataset\")\n","        train_dataset = raw_datasets[\"train\"]\n","        if data_args.max_train_samples is not None:\n","            # We will select sample from whole data if argument is specified\n","            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n","            train_dataset = train_dataset.select(range(max_train_samples))\n","        # Create train feature from dataset\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                prepare_train_features,\n","                batched=True,\n","                num_proc=data_args.preprocessing_num_workers,\n","                remove_columns=column_names,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        if data_args.max_train_samples is not None:\n","            # Number of samples might increase during Feature Creation, We select only specified max samples\n","            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n","            train_dataset = train_dataset.select(range(max_train_samples))\n","\n","    # Validation preprocessing\n","    def prepare_validation_features(examples):\n","        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","        # left whitespace\n","        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n","\n","        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","        # in one example possible giving several features when a context is long, each of those features having a\n","        # context that overlaps a bit the context of the previous feature.\n","        tokenized_examples = tokenizer(\n","            examples[question_column_name if pad_on_right else context_column_name],\n","            examples[context_column_name if pad_on_right else question_column_name],\n","            truncation=\"only_second\" if pad_on_right else \"only_first\",\n","            max_length=max_seq_length,\n","            stride=data_args.doc_stride,\n","            return_overflowing_tokens=True,\n","            return_offsets_mapping=True,\n","            padding=\"max_length\" if data_args.pad_to_max_length else False,\n","        )\n","\n","        # Since one example might give us several features if it has a long context, we need a map from a feature to\n","        # its corresponding example. This key gives us just that.\n","        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","\n","        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n","        # corresponding example_id and we will store the offset mappings.\n","        tokenized_examples[\"example_id\"] = []\n","\n","        for i in range(len(tokenized_examples[\"input_ids\"])):\n","            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","            sequence_ids = tokenized_examples.sequence_ids(i)\n","            context_index = 1 if pad_on_right else 0\n","\n","            # One example can give several spans, this is the index of the example containing this span of text.\n","            sample_index = sample_mapping[i]\n","            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n","\n","            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n","            # position is part of the context or not.\n","            tokenized_examples[\"offset_mapping\"][i] = [\n","                (o if sequence_ids[k] == context_index else None)\n","                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n","            ]\n","\n","        return tokenized_examples\n","\n","    if training_args.do_eval:\n","        if \"validation\" not in raw_datasets:\n","            raise ValueError(\"--do_eval requires a validation dataset\")\n","        eval_examples = raw_datasets[\"validation\"]\n","        if data_args.max_eval_samples is not None:\n","            # We will select sample from whole data\n","            max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)\n","            eval_examples = eval_examples.select(range(max_eval_samples))\n","        # Validation Feature Creation\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            print(\"******I am in the with statement******\") # Debugging\n","            eval_dataset = eval_examples.map(\n","                prepare_validation_features,\n","                batched=True,\n","                num_proc=data_args.preprocessing_num_workers,\n","                remove_columns=column_names,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","        if data_args.max_eval_samples is not None:\n","            # During Feature creation dataset samples might increase, we will select required samples again\n","            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n","            eval_dataset = eval_dataset.select(range(max_eval_samples))\n","\n","    if training_args.do_predict:\n","        if \"test\" not in raw_datasets:\n","            raise ValueError(\"--do_predict requires a test dataset\")\n","        predict_examples = raw_datasets[\"test\"]\n","        if data_args.max_predict_samples is not None:\n","            # We will select sample from whole data\n","            predict_examples = predict_examples.select(range(data_args.max_predict_samples))\n","        # Predict Feature Creation\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_examples.map(\n","                prepare_validation_features,\n","                batched=True,\n","                num_proc=data_args.preprocessing_num_workers,\n","                remove_columns=column_names,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","        if data_args.max_predict_samples is not None:\n","            # During Feature creation dataset samples might increase, we will select required samples again\n","            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n","            predict_dataset = predict_dataset.select(range(max_predict_samples))\n","\n","    # Data collator\n","    # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n","    # collator.\n","    data_collator = (\n","        default_data_collator\n","        if data_args.pad_to_max_length\n","        else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n","    )\n","\n","    # Post-processing:\n","    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n","        # Post-processing: we match the start logits and end logits to answers in the original context.\n","        predictions = postprocess_qa_predictions(\n","            examples=examples,\n","            features=features,\n","            predictions=predictions,\n","            version_2_with_negative=data_args.version_2_with_negative,\n","            n_best_size=data_args.n_best_size,\n","            max_answer_length=data_args.max_answer_length,\n","            null_score_diff_threshold=data_args.null_score_diff_threshold,\n","            output_dir=training_args.output_dir,\n","            log_level=log_level,\n","            prefix=stage,\n","        )\n","        # Format the result to the format the metric expects.\n","        if data_args.version_2_with_negative:\n","            formatted_predictions = [\n","                {\"id\": str(k), \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n","            ]\n","        else:\n","            formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in predictions.items()]\n","\n","        references = [{\"id\": str(ex[\"id\"]), \"answers\": ex[answer_column_name]} for ex in examples]\n","        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n","\n","    metric = evaluate.load(\n","        \"squad_v2\" if data_args.version_2_with_negative else \"squad\", cache_dir=model_args.cache_dir\n","    )\n","\n","    def compute_metrics(p: EvalPrediction):\n","        return metric.compute(predictions=p.predictions, references=p.label_ids)\n","\n","    # Initialize our Trainer\n","    trainer = QuestionAnsweringTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        eval_examples=eval_examples if training_args.do_eval else None,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        post_process_function=post_processing_function,\n","        compute_metrics=compute_metrics,\n","        callbacks=[metrics_logger],\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate()\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        results = trainer.predict(predict_dataset, predict_examples)\n","        metrics = results.metrics\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","    kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"question-answering\"}\n","    if data_args.dataset_name is not None:\n","        kwargs[\"dataset_tags\"] = data_args.dataset_name\n","        if data_args.dataset_config_name is not None:\n","            kwargs[\"dataset_args\"] = data_args.dataset_config_name\n","            kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n","        else:\n","            kwargs[\"dataset\"] = data_args.dataset_name\n","\n","    if training_args.push_to_hub:\n","        trainer.push_to_hub(**kwargs)\n","    else:\n","        trainer.create_model_card(**kwargs)\n","\n","\n","def _mp_fn(index):\n","    # For xla_spawn (TPUs)\n","    main()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","source":["# Skip to the End\n","Putting here to avoid having to scroll"],"metadata":{"id":"ncEHy3Q5LQSJ"}},{"cell_type":"code","source":[],"metadata":{"id":"PYYw-CTtLQBp"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"308259065b8b493b87136659d3256b00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ebb34079eab46cab5a95a7fab546f67","IPY_MODEL_d4e9c0ccbe584aa694917290976062d1","IPY_MODEL_acf3d247c7cf4e9195339e3bb5e29456"],"layout":"IPY_MODEL_aa742d4fe1b644419a4893547be8311c"}},"8ebb34079eab46cab5a95a7fab546f67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91c481c1150a455cb6ec94b4a8eafc96","placeholder":"​","style":"IPY_MODEL_1d0dc4710e7248b29be8d5a8415b45ad","value":"Running tokenizer on validation dataset: 100%"}},"d4e9c0ccbe584aa694917290976062d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_430bfe6e5ac44b3793083f18dcf8661d","max":755,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8da2cbd08cfc413f95373d1dd18f19db","value":755}},"acf3d247c7cf4e9195339e3bb5e29456":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a7f4e02cebd42419626c07aa630e803","placeholder":"​","style":"IPY_MODEL_516a30b2a9ea496884d83019e5b4773d","value":" 755/755 [00:00&lt;00:00, 1013.76 examples/s]"}},"aa742d4fe1b644419a4893547be8311c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91c481c1150a455cb6ec94b4a8eafc96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d0dc4710e7248b29be8d5a8415b45ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"430bfe6e5ac44b3793083f18dcf8661d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8da2cbd08cfc413f95373d1dd18f19db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a7f4e02cebd42419626c07aa630e803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"516a30b2a9ea496884d83019e5b4773d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae24a52c3026423abeb218f9e398fd82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d305fbb24564b4c8d74a0bdad998e47","IPY_MODEL_b2197d53ca694b13806e17178380f327","IPY_MODEL_ad7ec11cea374a95be1c20c1a40fb052"],"layout":"IPY_MODEL_98d6a00fafeb40928686a696fee93274"}},"0d305fbb24564b4c8d74a0bdad998e47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5968b0ca470e49d4b5f0766a150e9dec","placeholder":"​","style":"IPY_MODEL_e1a8fe0a0e384f1caac8f8c7193aaf88","value":"100%"}},"b2197d53ca694b13806e17178380f327":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f01d4194a4e4cabb4ae68f32bd196d3","max":755,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0ac1f4c04d346e39ec0e0ce72ed7770","value":755}},"ad7ec11cea374a95be1c20c1a40fb052":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1b4f4bc675d478ab197b4009fc12daa","placeholder":"​","style":"IPY_MODEL_6fb8b1888874464eb171e9bb1c593ae7","value":" 755/755 [00:03&lt;00:00, 308.07it/s]"}},"98d6a00fafeb40928686a696fee93274":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5968b0ca470e49d4b5f0766a150e9dec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1a8fe0a0e384f1caac8f8c7193aaf88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f01d4194a4e4cabb4ae68f32bd196d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ac1f4c04d346e39ec0e0ce72ed7770":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1b4f4bc675d478ab197b4009fc12daa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fb8b1888874464eb171e9bb1c593ae7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}