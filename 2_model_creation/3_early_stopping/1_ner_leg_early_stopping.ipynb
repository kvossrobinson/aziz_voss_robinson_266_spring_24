{"cells":[{"cell_type":"markdown","source":["# README\n","\n","**Purpose**\n","\n","Fine tune the pre-trained HuggingFace RoBERTa base on domain-specific NER with the dataset of your choice\n","\n","**How-To**\n","\n","Make any updates in the \"Update Each Run\" section before running all cells. Places to take particular care:\n","\n","\n","*   output_model - If you forget to change this and have overwrite set to True, you will erase whatever was previously in the folder.\n","*   overwrite - This must be set to False if you are training from a checkpoint.\n","*   name - I'm not sure this is getting used, but I've been updating it anyway.\n","\n"],"metadata":{"id":"RBt0eAlgBMu2"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"bPnMEvC_9HEz"}},{"cell_type":"markdown","source":["## Will Need Regular Changes"],"metadata":{"id":"66H42RDz9Hfa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UK37MFQN2rxO"},"outputs":[],"source":["## Training Args\n","\n","# Where to put the model outputs\n","output_path = '/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based'\n","\n","# Are you training?\n","do_train_flag = True\n","\n","# Are you evaluating?\n","do_eval_flag = True\n","\n","# Number of train epochs\n","epoch_count = 20 # Should definitely be high enough to test the early stopping\n","\n","# Number of save steps\n","save_step_count = 1000\n","\n","# Number of eval steps\n","eval_step_count = 1000 # Keep same as logging_steps\n","\n","# Number of logging steps\n","logging_step_count = eval_step_count # Keep same as logging_steps\n","\n","# Overwrite Output Dir - MUST be False if you're training from a checkpoint\n","overwrite = True\n","\n","# Run Name\n","name = '20 epoch NER for early stopping with EarlyStoppingCallback code'"]},{"cell_type":"markdown","source":["## Will *Not* Need Regular Changes"],"metadata":{"id":"7TIABThw9H_x"}},{"cell_type":"code","source":["## Data Args\n","\n","# Path to get the domain-specific NER data\n","data_path = '/content/drive/My Drive/266/source_of_truth/data/'\n","\n","## Preprocessing within Main\n","train_tag_scheme = ['B-ACTOR',\n","                    'B-CHARACTER',\n","                    'B-DIRECTOR',\n","                    'B-GENRE',\n","                    'B-PLOT',\n","                    'B-RATING',\n","                    'B-RATINGS_AVERAGE',\n","                    'B-REVIEW',\n","                    'B-SONG',\n","                    'B-TITLE',\n","                    'B-TRAILER',\n","                    'B-YEAR',\n","                    'I-ACTOR',\n","                    'I-CHARACTER',\n","                    'I-DIRECTOR',\n","                    'I-GENRE',\n","                    'I-PLOT',\n","                    'I-RATING',\n","                    'I-RATINGS_AVERAGE',\n","                    'I-REVIEW',\n","                    'I-SONG',\n","                    'I-TITLE',\n","                    'I-TRAILER',\n","                    'I-YEAR',\n","                    'O']"],"metadata":{"id":"WNAamZD1_-pA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22828,"status":"ok","timestamp":1712534382216,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"},"user_tz":300},"id":"jnfsWsXvRKXA","outputId":"68c09e8e-c3e3-4416-f132-b561a9e8f2e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9dp9sY4Tgtf","executionInfo":{"status":"ok","timestamp":1712534508506,"user_tz":300,"elapsed":126294,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"outputId":"75442384-5dbc-4f13-c1de-c9bbda4f8b53"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q evaluate\n","!pip install -q datasets\n","!pip install -q accelerate -U\n","!pip install -q seqeval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0taF9kX5RTXM"},"outputs":[],"source":["import logging\n","import os\n","import sys\n","import warnings\n","from dataclasses import dataclass, field\n","from typing import Optional, Tuple, Union, List, Dict\n","\n","import datasets\n","import evaluate\n","import numpy as np\n","from datasets import ClassLabel, load_dataset\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForTokenClassification,\n","    AutoTokenizer,\n","    DataCollatorForTokenClassification,\n","    HfArgumentParser,\n","    PretrainedConfig,\n","    PreTrainedTokenizerFast,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n","    RobertaPreTrainedModel, RobertaModel, RobertaConfig\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry, ModelOutput\n","from transformers.utils.versions import require_version\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"]},{"cell_type":"markdown","source":["# Custom Model"],"metadata":{"id":"AV2YxQVKEh6r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DeDuN5DT2yU"},"outputs":[],"source":["@dataclass\n","class TokenClassifierOutput(ModelOutput):\n","    \"\"\"\n","    Base class for outputs of token classification models.\n","\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n","            Classification loss.\n","        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n","            Classification scores (before SoftMax).\n","        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","\n","            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n","        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","\n","            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n","            heads.\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n","\n","class RobertaForTokenClassification(RobertaPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            # move labels to correct device to enable model parallelism\n","            labels = labels.to(logits.device)\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","          output = (logits,) + outputs[2:]\n","          return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QVmnfY_T-0s"},"outputs":[],"source":["# Setting logger\n","logger = logging.getLogger(__name__)\n","TRAINED_MODEL = None"]},{"cell_type":"markdown","metadata":{"id":"TYxaK6knRUGj"},"source":["# Classes"]},{"cell_type":"markdown","source":["## MetricsLogger"],"metadata":{"id":"OI5VOF0e9cvu"}},{"cell_type":"code","source":["from transformers import TrainerCallback\n","\n","class MetricsLogger(TrainerCallback):\n","    \"A logger that will store metrics after each evaluation.\"\n","    def __init__(self):\n","        self.metrics = []\n","\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            # You can add more metrics here if needed\n","            self.metrics.append({\n","                \"step\": state.global_step,\n","                \"loss\": logs.get(\"loss\", None),\n","                \"eval_loss\": logs.get(\"eval_loss\", None),\n","            })\n","\n","# Initialize your logger\n","metrics_logger = MetricsLogger()\n"],"metadata":{"id":"XHoSARQ8ttLz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ModelArguments"],"metadata":{"id":"RfWh61M99e19"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wx7lqvRRUOP"},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    token: str = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n","                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n","            )\n","        },\n","    )\n","    use_auth_token: bool = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n","        },\n","    )\n","    trust_remote_code: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": (\n","                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n","                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n","                \"execute code present on the Hub on your local machine.\"\n","            )\n","        },\n","    )\n","    ignore_mismatched_sizes: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\n","    )"]},{"cell_type":"markdown","source":["## DataTrainingArguments"],"metadata":{"id":"cVig-fx89ghX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXPu2alXUI0Y"},"outputs":[],"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n","    dataset_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )\n","    dataset_config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n","    )\n","    train_file: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n","    )\n","    validation_file: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n","    )\n","    test_file: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n","    )\n","    text_column_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n","    )\n","    label_column_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","    preprocessing_num_workers: Optional[int] = field(\n","        default=None,\n","        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n","    )\n","    max_seq_length: int = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n","                \"than this will be truncated, sequences shorter will be padded.\"\n","            )\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": (\n","                \"Whether to pad all samples to model maximum sentence length. \"\n","                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n","                \"efficient on GPU but very bad for TPU.\"\n","            )\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": (\n","                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","                \"value if set.\"\n","            )\n","        },\n","    )\n","    label_all_tokens: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": (\n","                \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n","                \"one (in which case the other tokens will have a padding index).\"\n","            )\n","        },\n","    )\n","    return_entity_level_metrics: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n","    )\n","\n","    def __post_init__(self):\n","        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n","            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n","        else:\n","            if self.train_file is not None:\n","                extension = self.train_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n","            if self.validation_file is not None:\n","                extension = self.validation_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n","        self.task_name = self.task_name.lower()"]},{"cell_type":"markdown","metadata":{"id":"UPrbWKuIRU65"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"ba67RcGQRVIC"},"source":["## Creating Args"]},{"cell_type":"code","source":["# Necessary for early stopping\n","from transformers import EarlyStoppingCallback"],"metadata":{"id":"Ni1BTDen_gDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EowQUcAVOzJ","executionInfo":{"status":"ok","timestamp":1712534592318,"user_tz":300,"elapsed":514,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"091c462c-90d5-4555-9713-d766641fb449"},"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO|training_args.py:1902] 2024-04-08 00:03:11,495 >> PyTorch: setting up devices\n","[INFO|training_args.py:1611] 2024-04-08 00:03:11,529 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# Creating instances of the three required classes\n","\n","model_args = ModelArguments(model_name_or_path = \"roberta-base\")\n","\n","data_args = DataTrainingArguments(train_file = data_path + 'plain_training.json',\n","                                  validation_file = data_path + 'plain_val.json',\n","                                  test_file = data_path + 'plain_test.json',\n","                                  max_seq_length = 384,\n","                                  overwrite_cache = True\n","                                  )\n","\n","training_args = TrainingArguments(output_dir = output_path,\n","                                  do_train = do_train_flag,\n","                                  do_eval = do_eval_flag,\n","                                  num_train_epochs = epoch_count,\n","                                  save_strategy = \"epoch\",\n","                                  evaluation_strategy = \"epoch\",\n","                                  save_total_limit = 2,\n","                                  per_device_train_batch_size = 16,\n","                                  per_device_eval_batch_size = 10,\n","                                  overwrite_output_dir = overwrite,\n","                                  run_name = name,\n","                                  load_best_model_at_end = True # Adding for early stopping\n","                                  )"]},{"cell_type":"markdown","metadata":{"id":"qUtQfOn4RVWC"},"source":["## Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzzG5hM-RVc4","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ceb61cc938124f6fb3198a33470bfec5","550713d481b645c5bddcdd14de0505d7","b0ba69818534497cbb8a27a35a612562","aa1a6b63d5c5454ebacf27fecad82dbb","35a8678261be4b0fb55a8fca247ed4eb","deedb81a744045af865f9b462a92e257","a35cd32978ac438282def46de97b6e97","ab0ba79747534c5e94907f40f77bbda7","2da5c8cc09834a2ba6d33274337b4040","10545366518b414384fd8246d4477272","597d3450225841a0ad878a1f4fd361ac","c3e3b13f71ac46c0854a8f24895978a7","046cfbe360964d049a5bdaaa546cd24a","bdce68955a3447e9817d56df3040bee3","8aba4dc73c4e4245acf313d573c0a298","be6297fdd43c4f968cc409cf7925d753","624015d135184c36a0c6fb8ed116f5ee","e4c78401f60442c2928f8164143163e3","889f392b1c9f4c72ba540e54797d4e24","db11d811bb21429c91b6ac7fabd94f43","3dc26093f4d648cf97179cb5e2dc3f02","2ffd2d906d4c414e8df05c187ac86f11"]},"executionInfo":{"status":"ok","timestamp":1712535178351,"user_tz":300,"elapsed":584192,"user":{"displayName":"Katherine Voss-Robinson","userId":"10124239414397994450"}},"outputId":"82615d8a-c44b-4e7c-b511-6b4c297fcaaf"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/runs/Apr08_00-03-11_4073c8b8cad9,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=20,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=10,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=20 epoch NER for early stopping with EarlyStoppingCallback code,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=2,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-68b35fe6e3e85351\n","INFO:datasets.builder:Using custom data configuration default-68b35fe6e3e85351\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n","INFO:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n","[INFO|configuration_utils.py:728] 2024-04-08 00:03:14,802 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","[INFO|configuration_utils.py:791] 2024-04-08 00:03:14,806 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"ner\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|configuration_utils.py:728] 2024-04-08 00:03:14,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","[INFO|configuration_utils.py:791] 2024-04-08 00:03:14,921 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,926 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,930 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,932 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,935 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,938 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2046] 2024-04-08 00:03:14,939 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n","[INFO|configuration_utils.py:728] 2024-04-08 00:03:14,942 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","[INFO|configuration_utils.py:791] 2024-04-08 00:03:14,946 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:3257] 2024-04-08 00:03:15,141 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n","[INFO|modeling_utils.py:3982] 2024-04-08 00:03:15,825 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3994] 2024-04-08 00:03:15,829 >> Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on train dataset:   0%|          | 0/7820 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb61cc938124f6fb3198a33470bfec5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-ed1248a04b60daab.arrow\n","INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-ed1248a04b60daab.arrow\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1955 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e3b13f71ac46c0854a8f24895978a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-783c160c7b1f96ba.arrow\n","INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68b35fe6e3e85351/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-783c160c7b1f96ba.arrow\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","[INFO|trainer.py:759] 2024-04-08 00:03:19,570 >> The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:1812] 2024-04-08 00:03:19,605 >> ***** Running training *****\n","[INFO|trainer.py:1813] 2024-04-08 00:03:19,608 >>   Num examples = 7,820\n","[INFO|trainer.py:1814] 2024-04-08 00:03:19,610 >>   Num Epochs = 20\n","[INFO|trainer.py:1815] 2024-04-08 00:03:19,613 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1818] 2024-04-08 00:03:19,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1819] 2024-04-08 00:03:19,637 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1820] 2024-04-08 00:03:19,641 >>   Total optimization steps = 9,780\n","[INFO|trainer.py:1821] 2024-04-08 00:03:19,645 >>   Number of trainable parameters = 124,074,265\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2445' max='9780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2445/9780 08:32 < 25:38, 4.77 it/s, Epoch 5/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.231422</td>\n","      <td>0.865918</td>\n","      <td>0.880037</td>\n","      <td>0.872921</td>\n","      <td>0.942254</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.400500</td>\n","      <td>0.236956</td>\n","      <td>0.844321</td>\n","      <td>0.883065</td>\n","      <td>0.863259</td>\n","      <td>0.938211</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.213600</td>\n","      <td>0.224352</td>\n","      <td>0.880372</td>\n","      <td>0.882833</td>\n","      <td>0.881600</td>\n","      <td>0.947544</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.169100</td>\n","      <td>0.230598</td>\n","      <td>0.858016</td>\n","      <td>0.882600</td>\n","      <td>0.870134</td>\n","      <td>0.943801</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.136800</td>\n","      <td>0.244232</td>\n","      <td>0.877402</td>\n","      <td>0.893548</td>\n","      <td>0.885401</td>\n","      <td>0.946496</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[INFO|trainer.py:759] 2024-04-08 00:03:55,069 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:03:55,074 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:03:55,075 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:03:55,077 >>   Batch size = 10\n","/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","[INFO|trainer.py:3067] 2024-04-08 00:04:56,913 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-489\n","[INFO|configuration_utils.py:473] 2024-04-08 00:04:56,924 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-489/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:05:01,898 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-489/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:05:01,912 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-489/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:05:01,929 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-489/special_tokens_map.json\n","[INFO|trainer.py:759] 2024-04-08 00:05:41,017 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:05:41,023 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:05:41,025 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:05:41,027 >>   Batch size = 10\n","[INFO|trainer.py:3067] 2024-04-08 00:06:42,178 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-978\n","[INFO|configuration_utils.py:473] 2024-04-08 00:06:42,189 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-978/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:06:44,114 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-978/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:06:44,126 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-978/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:06:44,134 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-978/special_tokens_map.json\n","[INFO|trainer.py:759] 2024-04-08 00:07:20,482 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:07:20,487 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:07:20,489 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:07:20,491 >>   Batch size = 10\n","[INFO|trainer.py:3067] 2024-04-08 00:08:19,121 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1467\n","[INFO|configuration_utils.py:473] 2024-04-08 00:08:19,135 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1467/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:08:21,674 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1467/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:08:21,685 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1467/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:08:21,696 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1467/special_tokens_map.json\n","[INFO|trainer.py:3159] 2024-04-08 00:08:26,112 >> Deleting older checkpoint [/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/checkpoint-489] due to args.save_total_limit\n","[INFO|trainer.py:759] 2024-04-08 00:08:56,856 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:08:56,861 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:08:56,863 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:08:56,865 >>   Batch size = 10\n","[INFO|trainer.py:3067] 2024-04-08 00:09:55,534 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1956\n","[INFO|configuration_utils.py:473] 2024-04-08 00:09:55,545 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1956/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:09:57,906 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1956/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:09:58,360 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1956/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:09:58,367 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-1956/special_tokens_map.json\n","[INFO|trainer.py:3159] 2024-04-08 00:10:02,558 >> Deleting older checkpoint [/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/checkpoint-978] due to args.save_total_limit\n","[INFO|trainer.py:759] 2024-04-08 00:10:34,122 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:10:34,127 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:10:34,129 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:10:34,131 >>   Batch size = 10\n","[INFO|trainer.py:3067] 2024-04-08 00:11:38,530 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-2445\n","[INFO|configuration_utils.py:473] 2024-04-08 00:11:38,542 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-2445/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:11:42,792 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-2445/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:11:42,813 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-2445/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:11:42,856 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tmp-checkpoint-2445/special_tokens_map.json\n","[INFO|trainer.py:3159] 2024-04-08 00:11:52,034 >> Deleting older checkpoint [/content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/checkpoint-1956] due to args.save_total_limit\n","[INFO|trainer.py:2067] 2024-04-08 00:11:52,460 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2271] 2024-04-08 00:11:52,462 >> Loading best model from /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/checkpoint-1467 (score: 0.22435249388217926).\n","[INFO|trainer.py:3067] 2024-04-08 00:11:54,984 >> Saving model checkpoint to /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based\n","[INFO|configuration_utils.py:473] 2024-04-08 00:11:54,995 >> Configuration saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/config.json\n","[INFO|modeling_utils.py:2454] 2024-04-08 00:11:57,206 >> Model weights saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/model.safetensors\n","[INFO|tokenization_utils_base.py:2459] 2024-04-08 00:11:57,223 >> tokenizer config file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2468] 2024-04-08 00:11:57,240 >> Special tokens file saved in /content/drive/MyDrive/266/experiments/models/4_7_ner_early_stopping_code_based/special_tokens_map.json\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:759] 2024-04-08 00:11:57,355 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3376] 2024-04-08 00:11:57,365 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3378] 2024-04-08 00:11:57,366 >>   Num examples = 1955\n","[INFO|trainer.py:3381] 2024-04-08 00:11:57,376 >>   Batch size = 10\n"]},{"output_type":"stream","name":"stdout","text":["***** train metrics *****\n","  epoch                    =        5.0\n","  total_flos               =   444948GF\n","  train_loss               =     0.2101\n","  train_runtime            = 0:08:35.32\n","  train_samples            =       7820\n","  train_samples_per_second =    303.495\n","  train_steps_per_second   =     18.978\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='196' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [196/196 00:59]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_accuracy           =     0.9475\n","  eval_f1                 =     0.8816\n","  eval_loss               =     0.2244\n","  eval_precision          =     0.8804\n","  eval_recall             =     0.8828\n","  eval_runtime            = 0:01:00.45\n","  eval_samples            =       1955\n","  eval_samples_per_second =     32.336\n","  eval_steps_per_second   =      3.242\n"]},{"output_type":"stream","name":"stderr","text":["[INFO|modelcard.py:450] 2024-04-08 00:12:58,192 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.8803716608594657}, {'name': 'Recall', 'type': 'recall', 'value': 0.8828325180526438}, {'name': 'F1', 'type': 'f1', 'value': 0.8816003721795765}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9475444200439209}]}\n"]}],"source":["if model_args.use_auth_token is not None:\n","    warnings.warn(\n","        \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\",\n","        FutureWarning,\n","    )\n","    if model_args.token is not None:\n","        raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n","    model_args.token = model_args.use_auth_token\n","\n","# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n","# information sent is the one passed as arguments along with your Python/PyTorch versions.\n","send_example_telemetry(\"run_ner\", model_args, data_args)\n","\n","# Setup logging\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n",")\n","\n","if training_args.should_log:\n","    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n","    transformers.utils.logging.set_verbosity_info()\n","\n","log_level = training_args.get_process_log_level()\n","logger.setLevel(log_level)\n","datasets.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.enable_default_handler()\n","transformers.utils.logging.enable_explicit_format()\n","\n","# Log on each process the small summary:\n","logger.warning(\n","    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n","    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",")\n","logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","# Detecting last checkpoint.\n","last_checkpoint = None\n","if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","            \"Use --overwrite_output_dir to overcome.\"\n","        )\n","    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n","        logger.info(\n","            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","        )\n","\n","# Set seed before initializing model.\n","set_seed(training_args.seed)\n","\n","# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n","# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n","# (the dataset will be downloaded automatically from the datasets Hub).\n","#\n","# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n","# 'text' is found. You can easily tweak this behavior (see below).\n","#\n","# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n","# download the dataset.\n","if data_args.dataset_name is not None:\n","    # Downloading and loading a dataset from the hub.\n","    raw_datasets = load_dataset(\n","        data_args.dataset_name,\n","        data_args.dataset_config_name,\n","        cache_dir=model_args.cache_dir,\n","        token=model_args.token,\n","    )\n","else:\n","    data_files = {}\n","    if data_args.train_file is not None:\n","        data_files[\"train\"] = data_args.train_file\n","        extension = data_args.train_file.split(\".\")[-1]\n","\n","    if data_args.validation_file is not None:\n","        data_files[\"validation\"] = data_args.validation_file\n","        extension = data_args.validation_file.split(\".\")[-1]\n","    if data_args.test_file is not None:\n","        data_files[\"test\"] = data_args.test_file\n","        extension = data_args.test_file.split(\".\")[-1]\n","    raw_datasets = load_dataset(\n","        extension,\n","        data_files=data_files,\n","        cache_dir=model_args.cache_dir\n","        )\n","# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n","# https://huggingface.co/docs/datasets/loading_datasets.\n","\n","if training_args.do_train:\n","    column_names = raw_datasets[\"train\"].column_names\n","    features = raw_datasets[\"train\"].features\n","else:\n","    column_names = raw_datasets[\"validation\"].column_names\n","    features = raw_datasets[\"validation\"].features\n","\n","if data_args.text_column_name is not None:\n","    text_column_name = data_args.text_column_name\n","elif \"tokens\" in column_names:\n","    text_column_name = \"tokens\"\n","else:\n","    text_column_name = column_names[0]\n","\n","if data_args.label_column_name is not None:\n","    label_column_name = data_args.label_column_name\n","elif f\"{data_args.task_name}_tags\" in column_names:\n","    label_column_name = f\"{data_args.task_name}_tags\"\n","else:\n","    label_column_name = column_names[1]\n","\n","# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n","# unique labels.\n","def get_label_list(labels):\n","    unique_labels = set()\n","    for label in labels:\n","        unique_labels = unique_labels | set(label)\n","    label_list = list(unique_labels)\n","    label_list.sort()\n","    return label_list\n","\n","# If the labels are of type ClassLabel, they are already integers and we have the map stored somewhere.\n","# Otherwise, we have to get the list of labels manually.\n","\n","if data_args.dataset_name is not None: # Runs the following if using a Hugging Face DF\n","  labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n","  if labels_are_int:\n","      label_list = features[label_column_name].feature.names\n","      label_to_id = {i: i for i in range(len(label_list))}\n","  else:\n","      label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n","      label_to_id = {l: i for i, l in enumerate(label_list)}\n","else: # Runs the following if using our own data\n","  # This is far from ideal but I'm tired of fighting it so I'm hard coding this in\n","  label_list = train_tag_scheme\n","  label_to_id = {l: i for i, l in enumerate(label_list)}\n","\n","num_labels = len(label_list)\n","\n","# Load pretrained model and tokenizer\n","#\n","# Distributed training:\n","# The .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","config = AutoConfig.from_pretrained(\n","    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","    num_labels=num_labels,\n","    finetuning_task=data_args.task_name,\n","    cache_dir=model_args.cache_dir,\n","    revision=model_args.model_revision,\n","    token=model_args.token,\n","    trust_remote_code=model_args.trust_remote_code,\n",")\n","\n","tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n","if config.model_type in {\"bloom\", \"gpt2\", \"roberta\"}:\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        tokenizer_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=True,\n","        revision=model_args.model_revision,\n","        token=model_args.token,\n","        trust_remote_code=model_args.trust_remote_code,\n","        add_prefix_space=True,\n","    )\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        tokenizer_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=True,\n","        revision=model_args.model_revision,\n","        token=model_args.token,\n","        trust_remote_code=model_args.trust_remote_code,\n","    )\n","\n","model = RobertaForTokenClassification.from_pretrained(\n","  'roberta-base',\n","  config=config,\n","  cache_dir=model_args.cache_dir,\n",")\n","\n","\n","# Tokenizer check: this script requires a fast tokenizer.\n","if not isinstance(tokenizer, PreTrainedTokenizerFast):\n","    raise ValueError(\n","        \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n","        \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n","        \" this requirement\"\n","    )\n","\n","# Model has labels -> use them.\n","if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n","    if sorted(model.config.label2id.keys()) == sorted(label_list):\n","        # Reorganize `label_list` to match the ordering of the model.\n","        if labels_are_int:\n","            label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}\n","            label_list = [model.config.id2label[i] for i in range(num_labels)]\n","        else:\n","            label_list = [model.config.id2label[i] for i in range(num_labels)]\n","            label_to_id = {l: i for i, l in enumerate(label_list)}\n","    else:\n","        logger.warning(\n","            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n","            f\"model labels: {sorted(model.config.label2id.keys())}, dataset labels:\"\n","            f\" {sorted(label_list)}.\\nIgnoring the model labels as a result.\",\n","        )\n","\n","# Set the correspondences label/ID inside the model config\n","model.config.label2id = {l: i for i, l in enumerate(label_list)}\n","model.config.id2label = dict(enumerate(label_list))\n","\n","# Map that sends B-Xxx label to its I-Xxx counterpart\n","b_to_i_label = []\n","for idx, label in enumerate(label_list):\n","    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n","        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n","    else:\n","        b_to_i_label.append(idx)\n","\n","# Preprocessing the dataset\n","# Padding strategy\n","padding = \"max_length\" if data_args.pad_to_max_length else False\n","\n","# Tokenize all texts and align the labels with them.\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[text_column_name],\n","        padding=padding,\n","        truncation=True,\n","        max_length=data_args.max_seq_length,\n","        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n","        is_split_into_words=True,\n","    )\n","    labels = []\n","    for i, label in enumerate(examples[label_column_name]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label_to_id[label[word_idx]])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                if data_args.label_all_tokens:\n","                    label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n","                else:\n","                    label_ids.append(-100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","if training_args.do_train:\n","    if \"train\" not in raw_datasets:\n","        raise ValueError(\"--do_train requires a train dataset\")\n","    train_dataset = raw_datasets[\"train\"]\n","    if data_args.max_train_samples is not None:\n","        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n","        train_dataset = train_dataset.select(range(max_train_samples))\n","    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","        train_dataset = train_dataset.map(\n","            tokenize_and_align_labels,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","            desc=\"Running tokenizer on train dataset\",\n","        )\n","\n","if training_args.do_eval:\n","    if \"validation\" not in raw_datasets:\n","        raise ValueError(\"--do_eval requires a validation dataset\")\n","    eval_dataset = raw_datasets[\"validation\"]\n","    if data_args.max_eval_samples is not None:\n","        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n","        eval_dataset = eval_dataset.select(range(max_eval_samples))\n","    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","        eval_dataset = eval_dataset.map(\n","            tokenize_and_align_labels,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","            desc=\"Running tokenizer on validation dataset\",\n","        )\n","\n","if training_args.do_predict:\n","    if \"test\" not in raw_datasets:\n","        raise ValueError(\"--do_predict requires a test dataset\")\n","    predict_dataset = raw_datasets[\"test\"]\n","    if data_args.max_predict_samples is not None:\n","        max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n","        predict_dataset = predict_dataset.select(range(max_predict_samples))\n","    with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","        predict_dataset = predict_dataset.map(\n","            tokenize_and_align_labels,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","            desc=\"Running tokenizer on prediction dataset\",\n","        )\n","\n","# Data collator\n","data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n","\n","# Metrics\n","metric = evaluate.load(\"seqeval\", cache_dir=model_args.cache_dir)\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    if data_args.return_entity_level_metrics:\n","        # Unpack nested dictionaries\n","        final_results = {}\n","        for key, value in results.items():\n","            if isinstance(value, dict):\n","                for n, v in value.items():\n","                    final_results[f\"{key}_{n}\"] = v\n","            else:\n","                final_results[key] = value\n","        return final_results\n","    else:\n","        return {\n","            \"precision\": results[\"overall_precision\"],\n","            \"recall\": results[\"overall_recall\"],\n","            \"f1\": results[\"overall_f1\"],\n","            \"accuracy\": results[\"overall_accuracy\"],\n","        }\n","\n","# Adding for early stopping\n","early_stop = EarlyStoppingCallback(early_stopping_patience=2)\n","\n","# Initialize our Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset if training_args.do_train else None,\n","    eval_dataset=eval_dataset if training_args.do_eval else None,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[metrics_logger, early_stop], # Edited for early stopping\n",")\n","\n","# Training\n","if training_args.do_train:\n","    checkpoint = None\n","    if training_args.resume_from_checkpoint is not None:\n","        checkpoint = training_args.resume_from_checkpoint\n","    elif last_checkpoint is not None:\n","        checkpoint = last_checkpoint\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","    metrics = train_result.metrics\n","    trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","    max_train_samples = (\n","        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","    )\n","    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","    trainer.save_state()\n","\n","# Evaluation\n","if training_args.do_eval:\n","    logger.info(\"*** Evaluate ***\")\n","\n","    metrics = trainer.evaluate()\n","\n","    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","    trainer.log_metrics(\"eval\", metrics)\n","    trainer.save_metrics(\"eval\", metrics)\n","\n","# Predict\n","if training_args.do_predict:\n","    logger.info(\"*** Predict ***\")\n","\n","    predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    trainer.log_metrics(\"predict\", metrics)\n","    trainer.save_metrics(\"predict\", metrics)\n","\n","    # Save predictions\n","    output_predictions_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n","    if trainer.is_world_process_zero():\n","        with open(output_predictions_file, \"w\") as writer:\n","            for prediction in true_predictions:\n","                writer.write(\" \".join(prediction) + \"\\n\")\n","\n","kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"token-classification\"}\n","if data_args.dataset_name is not None:\n","    kwargs[\"dataset_tags\"] = data_args.dataset_name\n","    if data_args.dataset_config_name is not None:\n","        kwargs[\"dataset_args\"] = data_args.dataset_config_name\n","        kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n","    else:\n","        kwargs[\"dataset\"] = data_args.dataset_name\n","\n","if training_args.push_to_hub:\n","    trainer.push_to_hub(**kwargs)\n","else:\n","    trainer.create_model_card(**kwargs)\n","TRAINED_MODEL = model\n"]},{"cell_type":"markdown","metadata":{"id":"2whkQbPeZ6zI"},"source":["## Save Model Weights\n","For use in TDAPT\n","\n"]},{"cell_type":"code","source":["# Fetching Roberta specific weights\n","roberta_weights = {k: v for k, v in TRAINED_MODEL.state_dict().items() if k.startswith('roberta')}"],"metadata":{"id":"fIlI7sHxV-xn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(roberta_weights, output_path + '/ner_weights.pt') # These will always drop in the model output folder and use the same name"],"metadata":{"id":"WiboVmlxZAaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J9F1RuIPFyND"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"16nq_G0fn-Z5dXDvDuKNmtbgRtYFxujKP","timestamp":1712236929314},{"file_id":"1PQxl9bHrT02ZkfLlWdbbOxrlFZ-zDEop","timestamp":1710664142952}],"gpuType":"V100","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ceb61cc938124f6fb3198a33470bfec5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_550713d481b645c5bddcdd14de0505d7","IPY_MODEL_b0ba69818534497cbb8a27a35a612562","IPY_MODEL_aa1a6b63d5c5454ebacf27fecad82dbb"],"layout":"IPY_MODEL_35a8678261be4b0fb55a8fca247ed4eb"}},"550713d481b645c5bddcdd14de0505d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_deedb81a744045af865f9b462a92e257","placeholder":"​","style":"IPY_MODEL_a35cd32978ac438282def46de97b6e97","value":"Running tokenizer on train dataset: 100%"}},"b0ba69818534497cbb8a27a35a612562":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab0ba79747534c5e94907f40f77bbda7","max":7820,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2da5c8cc09834a2ba6d33274337b4040","value":7820}},"aa1a6b63d5c5454ebacf27fecad82dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10545366518b414384fd8246d4477272","placeholder":"​","style":"IPY_MODEL_597d3450225841a0ad878a1f4fd361ac","value":" 7820/7820 [00:01&lt;00:00, 5024.34 examples/s]"}},"35a8678261be4b0fb55a8fca247ed4eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deedb81a744045af865f9b462a92e257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35cd32978ac438282def46de97b6e97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab0ba79747534c5e94907f40f77bbda7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2da5c8cc09834a2ba6d33274337b4040":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10545366518b414384fd8246d4477272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"597d3450225841a0ad878a1f4fd361ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3e3b13f71ac46c0854a8f24895978a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_046cfbe360964d049a5bdaaa546cd24a","IPY_MODEL_bdce68955a3447e9817d56df3040bee3","IPY_MODEL_8aba4dc73c4e4245acf313d573c0a298"],"layout":"IPY_MODEL_be6297fdd43c4f968cc409cf7925d753"}},"046cfbe360964d049a5bdaaa546cd24a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_624015d135184c36a0c6fb8ed116f5ee","placeholder":"​","style":"IPY_MODEL_e4c78401f60442c2928f8164143163e3","value":"Running tokenizer on validation dataset: 100%"}},"bdce68955a3447e9817d56df3040bee3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_889f392b1c9f4c72ba540e54797d4e24","max":1955,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db11d811bb21429c91b6ac7fabd94f43","value":1955}},"8aba4dc73c4e4245acf313d573c0a298":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dc26093f4d648cf97179cb5e2dc3f02","placeholder":"​","style":"IPY_MODEL_2ffd2d906d4c414e8df05c187ac86f11","value":" 1955/1955 [00:00&lt;00:00, 5003.01 examples/s]"}},"be6297fdd43c4f968cc409cf7925d753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"624015d135184c36a0c6fb8ed116f5ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4c78401f60442c2928f8164143163e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"889f392b1c9f4c72ba540e54797d4e24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db11d811bb21429c91b6ac7fabd94f43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dc26093f4d648cf97179cb5e2dc3f02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffd2d906d4c414e8df05c187ac86f11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}